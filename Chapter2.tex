\chapter{Related Work}
\label{chapterlabel2}

There are three models related to our work, static topic model, Time-aware topic model and corresponding applications, with a rich literature available on those topics. In the following sections we will introduce each model. 

\section{Static Topic Model}
In recent years it becomes more difficult to search and discover the pinpoint information when more knowledge and information become available to people. The tool which can help us to search,classify and comprehend the great amount of information is needed. 

Static topic model is the way to help people automatically searching, organizing, understanding the vast amounts of information and electronic documents. By taking several collections of documents and using a series of algorithms, a set of latent hidden topics can be inferred  and discovered by topic modeling. Besides, the annotation information of to what extent each document contributes to those latent topics can be obtained, which bridges the documents and topics. People can use the annotations information to understand, classify, summarize and search the archives.

\subsection{Latent Dirichlet Allocation}

Blei et al. in \cite{blei2003latent}firstly proposed a generative model that models each document as a mixture of topics, each topic as a distribution over words and each word as derived from topics. In fact, only documents can be observed and other variables are hidden variables. Therefore inferring the hidden variables, for example, compute the distribution of topics, proportions and assignments conditioned on the documents is critical. In LDA, each document can be regarded as formed by the same shared topics but with different proportions. The hidden thematic structure can be visualized and then new data is generalized to fit into the structure by using LDA. 

Numerous researches have been extended based on Blei's work in 2003. LDA \cite{blei2003latent}model outperforms than other models because it hold the idea that documents are comprised of words drawn from several topics, instead of a singe topic, which obviously increased the computational cost of unsupervised estimation especially when people can only observe the words and have no hidden corresponding topics off hand. 

Matthew D. Hoffman, et al.\cite{hoffman2010online} provided a much faster online algorithm that can process a great amount of documents and can converge streaming collections of text for Latent Dirichlet Allocation. Anima Anandkumar, et al.\cite{anandkumar2012spectral} proposed a more simple and efficient learning algorithm, a spectral algorithm, which guarantees to find the parameters for Latent Dirichlet Allocation. Ian Porteous, et al.\cite{krestel2009latent} introduced a method which has a rapid improvement of the speed of LDA Gibbs sampling by taking advantage of organizing the computations in a better way, estimating a dynamic upper bound in an adaptive way and providing precisely equivalent samples.

TWILITE, developed by Younghoon Kim, et al.\cite{kim2014twilite}  using LDA model, is a recommendation system which can return the top tweets and provide to the user as well as the top users that can be followed by the certain user. When people tweet messages or add new friends, this posting process can be captured by TWILITE by constructing an latent Dirichlet allocation model and this process of network connection can be obtained by using matrix factorization. The new model can obtain the hidden corresponding topics  by analyzing both the contents of tweet messages and network relationships  and recommend to users, with faster speed and better performance.

\subsection{Author Topic Model}

Michal Rosen-Zvi,el al.\cite{rosen2004author} and Mark Steyvers,el al.\cite{steyvers2004probabilistic} described the author-topic (AT) model, a generative model for document collections, which models each document as a mixture of topics, like Latent Dirichlet Allocation \cite{blei2003latent}. Including authorship information and allowing the mixture weights for different topics to be determined by the authors of the document is the extension based on topic models. The content of documents and the interests of authors can be simultaneously modeled by AT model, which hold an idea that a probability distribution over topics is deemed to represent each author,  a probability distribution over words for that topic is deemed to represent each topic. From a mixture of each authors’ topic, the words are regarded as the result in a multi-author paper. A document with multiple authors can be regarded as a distribution over topics which is a mixture of the distributions associated with the authors. By learning the parameters of the innovative model, the relationships between authors, documents, topics, and words can be explored.

Numerous researches have been extended based on author-topic (AT) model. Andrew McCallum,el al.\cite{mccallum2005author} proposed an Author-Recipient-Topic (ART) model for networking message data, which the latent topics and the directed senders and receivers in social network can be captured  distinctly on both the author and one recipient of a message. The difference between AT and ART model is that the ART model considers not only author but also recipients, regarding experiments with Enron and Academic Email as mixture of topics. ART model is useful when processing with large bodies of networking message data, with better performance in finding topics conditioned on message sending relationships,  exploring in relationship of social roles and understanding the content of message data in order to make recommendations and prioritization.

Other extension based on author-topic (AT) model is a recommendation system for users. MICHAL ROSEN-ZVI,el al. \cite{rosen2010learning} applied the author-topic model on large text corpora, including papers, abstracts and emails. By analyzing the words expressed in a paper, latent topics extracting from documents and the names of the authors as well as utilizing the result of AT model, automatic recommendation system can be obtained to push recommendations with related topics and authors. Shuhui Jiang,el al. \cite{jiang2015author} proposed the personalized traveling recommendation system based on author topic model using collaborative filtering (ATCF) method. By processing and analyzing the profile of user and travel habit, the latent travel topics and travel preference can be obtained simultaneously and then make customized recommendations to users.

\section{Time-aware Topic Model}

The effect of time is not considered by static topic model, which is not adaptive for many evolution applications. As we know, a list of recent topics usually sorted by popularity or freshness.  For example, the search volume of the topic of Apple always increased abruptly when Apple Inc releases the new products. Obviously, the time factor is critical and should be considered. Therefore time-aware topic model came into being, which models topics concerning time variation. In the following sections we will introduce three models related to time-aware topic model.

\subsection{Dynamic Topic Model}

Dynamic Topic Model (DTM)\cite{blei2006dynamic} captures the natural parameters in state space model, which models sequential topics drawn from the organized collections of documents in evolving way and takes advantage of Kalman filters and wavelet regression as variational approximations. DTM requires discretized time slices, namely, documents are classified by a period of time, for example, month by month, under which model a series of topics evolved from last month’s topics are arisen from each month’s documents. Therefore determining the length of the time span influences the performance of the Dynamic Topic Model. Tomoharu Iwata,el al. \cite{iwata2009topic} developed the method that tracks the topic changes by multi-scale time span to slove the problem. Dynamic Topic Model provides an innovative way when processing unstructured data in large amount.

Chong Wang,el al.\cite{wang2012continuous} developed the continuous time dynamic topic model (cDTM), which applies Brownian motion that is continuous generalization on a sequential collection of documents to model continuous-time topic evolution. It is noteworthy that the pattern of the topic can be extracted from the word which evolves over the collection of documents. Compared with DMT, the advantage of the cDMT is that time can be continuous and we can apply sparse variational inference to compare models in faster speed. 

Normally, we think that documents are the mixture of topics and we use the whole document level to compute the mixture, which means the topics are extracted from the entire collection of document with no need to specify where the topics occur in the document. John Canny,el al \cite{canny2006dynamic} described a new model which refers every word in the whole collection of document to compute the topic mixture. John brought the concept of topical segmentation in dynamic topic estimation that a document can be broken into several passages which contain only single topic and are different with adjacent passages. By taking estimation of per-word topic mixture distribution, John extended the dynamic topic model on document segmentation.

Tomoharu Iwata,el al. \cite{iwata2010online}described an online dynamic topic model with multi scale, which can extract  the latent topics from the collections of documents  in sequential and evolving way.  In real word, the way topics evolve relates with multiple timescales. For instance, some network glossaries just appear and disappear in very short time with the short-timescale dependency, while other words can be spread and used over hundred years with the long-timescale dependency. Therefore the distribution of the of the current topic over words are related with the generation of the distribution of words on the multi scale in the previous epoch. The evolution of topics at various resolutions of timescales can be analyzed by the use of multiscale dynamic topic model(MDTM), which has a better performance both in effectiveness and computational efficiency in collections of the documents with timestamps.

\subsection{Topic Over Time Model}
Drawn from the LDA topic model, Xuerui Wang,el al. \cite{wang2006topics} provided Topic Over Time (TOT) model, which concerns not only the structure of data, but also the variation of the structure over times. The difference between TOT and DTM is that topics are related with a continuous and discrete distribution over timestamps respectively, where the discretization results from associating with each topic a continuous distribution over time can be avoided. Both word co-occurrences and temporal information have an influence on topic generation, where the timestamp values plays an important role in TOT model. The advantage of TOT is that the prediction of absolute time values when given an unstamped document and topic distributions given a timestamp can be done in long-range dependency, as well as deducing the risk of improper division in one topic where has a gap in its appearance by the use of Markov model.

Extended from TOT model, Avinava Dubey,el al. \cite{dubey2013nonparametric} described a nonparametric Topics over Time (npTOT) model, which incorporate with time-varying topics. Unlike the TOT model, the first distinction of npTOT is that an unbounded number of topics is allowed, which can reach the peak of popularity in an unbounded number of times. The second distinction of npTOT is that the correlations between the time changes in topic popularity is induced, where the flexible distribution over the temporal changes over the topics’ popularity can be captured with relative low computational complexity. Therefore, in npTOT model the related topics performance in similar manners. Timestamp can be regarded as a random variable when combining it with text for document jointly computation by the use of TOT model. In that way, non-Markovian dynamics can be incorporated with reasonable inference requirements, namely, the data can be processed without covariate information within the framework. In npTOT model, the pairs of text and time, document and timestamp can be regarded as exchangeable variable, which can be used when constructing a Gibbs sampling scheme.

Twitter is the most widely used social media, where the millions of real-time tweets are published. In order to analyze and visualize a high-level overview of a Twitter stream, Sana Malik,el al.\cite{malik2013topicflow} applied real-time twitter data on binned topic model (statistical topic modeling and alignment) where automatically generated topics and TopicFlow can be organized by related twitter data. The “topics” that extracted from a collection of documents can be discovered by statistical topic modeling. The binned topic model, an innovative extension from statistical topic modeling, concerns the variation in topics over time and identifies the appearance of fresh topics, where the deeper insight from the data can be analyzed from the combination of the topics. The natural attribution of twitter data is continuously evolving and diverse, which can be simulated by binned topic model. The distinction of the topics generated from binned topic model is that adjacent time slice of data does not directly correspond to the other topics, with the next step of alignment. The emergence, convergence, and divergence of complex topics in a Twitter stream can be visualized by TopicFlow.

Another extension such as Stephen W. Thomas,el al. \cite{thomas2010validating} used the topic models to analyze the evolution of software, and found that spikes and drops in the metric values of topics can be characterized by the topic evolutions. David Hall, el al. \cite{hall2008studying} studied about the development of ideas in a scientific field by the use of topic models, which found the trend of rise, steady increase and sharp decline and verified the strength of each topic in different time period.

\subsection{Topic Tracking Model}

\subsection{XXX}
\subsection{XXX}

\section{Applications}
\subsection{XXX}
\subsection{XXX}
\subsection{XXX}
\subsection{Applications on News}


% This just dumps some pseudolatin in so you can see some text in place.

 this paper describes dynamic topic model \cite{blei2006dynamic}
this paper is author topic \cite{rosen2004author}